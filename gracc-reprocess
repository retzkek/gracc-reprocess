#!/bin/env python
"""
Reprocess records by extracting original XML and sending them 
back through the GRACC collector.

* Get records from Elasticsearch indices
* extract original XML
* package into bundles
* send to collector
"""

from argparse import ArgumentParser
import logging
import requests
import traceback
from elasticsearch import Elasticsearch, RequestsHttpConnection


logger = logging.getLogger(__name__)

class Processor(object):
    def __init__(self, index, url, size=500, close=False, timeout=60):
        self.index = index
        self.url = url
        self.size = size
        self.close = close

        self.client = Elasticsearch(timeout=timeout,connection_class=RequestsHttpConnection)

    def process(self):
        for idx in self.get_indices(self.index):
            try:
                logger.log(99,'Processing index %s'%idx)
                self.process_index(idx)
            except:
                logger.error('Exception while processing index %s'%idx)
                traceback.print_exc()
                continue

    def process_index(self,index):
        s = self.client.search(index=index, 
                _source=["message"],
                scroll="5m",
                size=self.size)
        all_hits = s['hits']['total']
        logger.log(99,'Processing %d recordss'%all_hits)
        self.send_records(s['hits']['hits'])
        sent_hits = len(s['hits']['hits'])
        logger.info('sent %d/%d records'%(sent_hits,all_hits))
        while True:
            s = self.client.scroll(scroll_id=s['_scroll_id'],scroll="5m")
            if len(s['hits']['hits']) == 0:
                break
            self.send_records(s['hits']['hits'])
            sent_hits += len(s['hits']['hits'])
            logger.info('sent %d/%d records'%(sent_hits,all_hits))
        logger.log(99,'Processing complete. Sent %d/%d records'%(sent_hits,all_hits))
        if self.close:
            logger.log(99,'Closing index %s'%index)
            self.client.indices.close(index)


    def get_indices(self, index_pattern):
        idxs = self.client.indices.get_settings(index=index_pattern)
        return idxs.keys()

    def make_bundle(self, r):
        b = ''
        for s in r:
            b += 'replication|%s|%s||' % (s['_source']['message'],s['_source']['message'])
        return b

    def send_records(self, r):
        b = self.make_bundle(r)
        p = {
            'command': 'update',
            'from': 'gracc-reprocess',
            'bundlesize': len(r),
            'arg1': b,
        }
        url = self.url.rstrip('/')+'/gratia-servlets/rmi'

        logger.debug('Posting bundle to URL %s\n%s'%(url,p))
        res = requests.post(url, data=p)
        res.raise_for_status()


if __name__=="__main__":
    parser = ArgumentParser(description='Reprocess GRACC records through collector')
    parser.add_argument('index',help='source index pattern')
    parser.add_argument('url',help='GRACC collector URL')
    parser.add_argument('--size',help='Record batch size (default 500)',default=500)
    parser.add_argument('--close',help='Close index when done processing',action='store_true')
    parser.add_argument('--debug',help='Enable debug logging',action='store_true')
    parser.add_argument('--quiet',help='Disable most logging (ignored if --debug specified)',action='store_true')

    args = parser.parse_args()
    
    logfmt = "%(asctime)s [%(levelname)s %(name)s] %(message)s"
    if args.debug:
        logging.basicConfig(level=logging.DEBUG, format=logfmt)
    elif args.quiet:
        logging.basicConfig(level=logging.ERROR, format=logfmt)
    else:
        logging.basicConfig(level=logging.INFO, format=logfmt)
    logging.addLevelName(99,'UBER')

    processor = Processor(args.index, args.url, args.size, args.close)
    processor.process()
