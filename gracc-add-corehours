#!/bin/env python
"""
Add CoreHours field to raw documents that are missing it.
"""

from argparse import ArgumentParser
import logging
import requests
import traceback
from elasticsearch import Elasticsearch, RequestsHttpConnection
from elasticsearch.helpers import bulk


logger = logging.getLogger(__name__)

class Processor(object):
    def __init__(self, index, url, size=500, close=False, timeout=300):
        self.index = index
        self.url = url
        self.size = size
        self.close = close

        self.client = Elasticsearch(timeout=timeout,connection_class=RequestsHttpConnection)

    def process(self):
        for idx in self.get_indices(self.index):
            try:
                logger.log(99,'Processing index %s'%idx)
                self.process_index(idx)
            except:
                logger.error('Exception while processing index %s'%idx)
                traceback.print_exc()
                continue

    def process_index(self,index):
        s = self.client.search(index=index, 
                q='NOT _exists_:CoreHours',
                _source=['WallDuration','Processors'],
                scroll="5m",
                size=self.size)
        all_hits = s['hits']['total']
        logger.log(99,'Processing %d records'%all_hits)
        self.process_records(s['hits']['hits'])
        sent_hits = len(s['hits']['hits'])
        logger.info('processed %d/%d records'%(sent_hits,all_hits))
        while True:
            s = self.client.scroll(scroll_id=s['_scroll_id'],scroll="5m")
            if len(s['hits']['hits']) == 0:
                break
            self.process_records(s['hits']['hits'])
            sent_hits += len(s['hits']['hits'])
            logger.info('processed %d/%d records'%(sent_hits,all_hits))
        logger.log(99,'Processing complete. Processed %d/%d records'%(sent_hits,all_hits))
        if self.close:
            logger.log(99,'Closing index %s'%index)
            self.client.indices.close(index)


    def get_indices(self, index_pattern):
        idxs = self.client.indices.get_settings(index=index_pattern)
        return idxs.keys()

    def process_records(self, docs):
        #for doc in self.generate_updates(docs):
        #    logger.debug(doc)
        logger.debug(bulk(self.client, self.generate_updates(docs)))

    def generate_updates(self, docs):
        for doc in docs:
            wall = doc['_source'].get('WallDuration',0)
            proc = doc['_source'].get('Processors',1)
            del doc['_source']
            doc['_op_type'] = 'update'
            doc['doc'] = {'CoreHours':float(wall)*float(proc)/3600.0}
            yield doc


if __name__=="__main__":
    parser = ArgumentParser(description='Add CoreHours field to raw records')
    parser.add_argument('index',help='source index pattern')
    parser.add_argument('url',help='GRACC collector URL')
    parser.add_argument('--size',help='Record batch size (default 500)',default=500)
    parser.add_argument('--close',help='Close index when done processing',action='store_true')
    parser.add_argument('--debug',help='Enable debug logging',action='store_true')
    parser.add_argument('--quiet',help='Disable most logging (ignored if --debug specified)',action='store_true')

    args = parser.parse_args()
    
    logfmt = "%(asctime)s [%(levelname)s %(name)s] %(message)s"
    if args.debug:
        logging.basicConfig(level=logging.DEBUG, format=logfmt)
    elif args.quiet:
        logging.basicConfig(level=logging.ERROR, format=logfmt)
    else:
        logging.basicConfig(level=logging.INFO, format=logfmt)
    logging.addLevelName(99,'UBER')

    processor = Processor(args.index, args.url, args.size, args.close)
    processor.process()
